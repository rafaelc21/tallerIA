{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d937bf87-7a31-4d4f-97cb-e6ce19e075c3",
   "metadata": {},
   "source": [
    "# Usando la API de DeepSeek - Reasoner\n",
    "https://api-docs.deepseek.com/guides/reasoning_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ad77f-42a6-496f-98b5-262d67c7d6a1",
   "metadata": {},
   "source": [
    " üîç **¬øQu√© es DeepSeek Reasoner?**  \n",
    "DeepSeek es una plataforma de IA avanzada dise√±ada para ofrecer soluciones inteligentes y personalizadas en tiempo real.  \n",
    "Con modelos de lenguaje de √∫ltima generaci√≥n, DeepSeek te permite:  \n",
    "\n",
    "DeepSeek Reasoning Model es un modelo avanzado de razonamiento y generaci√≥n de lenguaje desarrollado por DeepSeek, \n",
    "dise√±ado para realizar tareas complejas que requieren comprensi√≥n profunda, l√≥gica y capacidad de inferencia.  \n",
    "A diferencia de los modelos de lenguaje tradicionales, est√° optimizado para:\n",
    "\n",
    "‚úÖ Razonamiento paso a paso  \n",
    "‚úÖ Alta precisi√≥n en tareas complejas  \n",
    "‚úÖ Contexto extendido (hasta 128K tokens)  \n",
    "‚úÖ Optimizaci√≥n para respuestas estructuradas    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d92e0-b9a2-482d-8b1e-5712c245db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa el cliente estandar de OpenAI, se conecta DIRECTAMENTE a los servicios de OpenAI como : GPT-3, GPT-4, DALL-E, etc.\n",
    "from openai import OpenAI\n",
    "# Librerias que permite interactuar con el Sistema Operativo. Administrar archivos, variables de entorno y m√°s.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d0224-4588-4cf5-b70d-de5c4c493024",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-ebed09dc4b2242bebd1e21cd3be686d3\"  # ¬°Mant√©n tus claves seguras!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25451d13",
   "metadata": {},
   "source": [
    "### Deepseek-Chat\n",
    "Dise√±ado para respuestas r√°pidas y directas, ideal para interacciones conversacionales est√°ndar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc1047-7a37-40af-aebe-56f90296c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],  # Clave API de OpenAI.\n",
    "    base_url=\"https://api.deepseek.com\"  # URL base de la API. Por defecto https://api.openai.com/v1, puedes cambiarla si est√°s utilizando otra solucion.\n",
    "    )\n",
    "\n",
    "# El metodo client.chat.completions.create es una funcion proporcionada por la biblioteca OpenAI\n",
    "# Facilita la interaccion con modelos de lenguaje basados en chat, como GPT-4 u otros. \n",
    "# Permite enviar una solicitud a la API de OpenAI (o a una API compatible, como DeepSeek) para generar respuestas de chat en formato de conversacion.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",  # Nombre del modelo a utilizar como: \"gpt-4-turbo\", \"deepseek-chat\"\n",
    "    # Lista de mensajes del chat en formato [{role: \"user\", content: \"Hola\"}]\n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": \"Eres un profesor de matematicas\"},\n",
    "        {\"role\": \"user\", \"content\": \"9.11 o 9.8, cual es mayor?\"}\n",
    "    ],    \n",
    "    stream=False,  # (True) la respuesta del modelo se env√≠a en fragmentos (streaming) y (False) la respuesta del modelo como un bloque completo.\n",
    "    #max_tokens= 50,  # L√≠mite m√°ximo de tokens en la respuesta generada.\n",
    "    temperature = 0.7,  # Controla la aleatoriedad. Rango (0 a 2). Respuestas m√°s deterministas (0.0 - 0.3) , respuestas muy aleatorias (1.3 - 2.0).\n",
    "    timeout=30 # Cancela la conexi√≥n si transcurre 30 segundos sin recibir la respuesta completa.\n",
    ")\n",
    "\n",
    "resultado = response.choices[0].message\n",
    "print(\"Respuesta:\", resultado.content)\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Tokens usados:\", response.usage.total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ac437",
   "metadata": {},
   "source": [
    "### Deepseek-Reasoner\n",
    "Optimizado para tareas que requieren un an√°lisis m√°s profundo y detallado, lo que resulta en tiempos de respuesta m√°s largos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc7d72-9649-4584-858c-167b8612e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"], \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"Eres un profesor\"},\n",
    "            {\"role\": \"user\", \"content\": \"9.11 o 9.8, cual es mayor?\"}\n",
    "           ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages= [\n",
    "        {\"role\": \"system\", \"content\": \"Eres un profesor\"},\n",
    "        {\"role\": \"user\", \"content\": \"9.11 o 9.8, cual es mayor?\"}\n",
    "],\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reasoning:\", reasoning_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b68039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Respuesta:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c608b",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"], \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"Eres un profesor\"},\n",
    "            {\"role\": \"user\", \"content\": \"9.11 o 9.8, cual es mayor?\"}\n",
    "           ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"========== RAZONAMIENTO EN STREAMING =============\")\n",
    "respuesta_actual = \"\"  # Acumulador para la respuesta\n",
    "\n",
    "for chunk in response:\n",
    "    if not chunk.choices:\n",
    "        continue\n",
    "    \n",
    "    delta = chunk.choices[0].delta\n",
    "    \n",
    "    if getattr(delta, 'reasoning_content', None):\n",
    "        print(delta.reasoning_content, end='', flush=True)\n",
    "    \n",
    "    if getattr(delta, 'content', None):\n",
    "        respuesta_actual += delta.content  # Acumula sin imprimir\n",
    "\n",
    "# Imprime la respuesta completa al final\n",
    "if respuesta_actual:\n",
    "    print(f\"\\n\\n========== RESPUESTA ==========\\n{respuesta_actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cb3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
